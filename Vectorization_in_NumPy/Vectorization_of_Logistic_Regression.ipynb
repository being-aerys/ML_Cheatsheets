{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of Logistic Regression\n",
    "This notebook illustrates how to perform vectorization of common calculations encountered in logistic regression using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Dataset and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431, 0.73199394],\n",
       "       [0.59865848, 0.15601864, 0.15599452],\n",
       "       [0.05808361, 0.86617615, 0.60111501],\n",
       "       [0.70807258, 0.02058449, 0.96990985],\n",
       "       [0.83244264, 0.21233911, 0.18182497],\n",
       "       [0.18340451, 0.30424224, 0.52475643],\n",
       "       [0.43194502, 0.29122914, 0.61185289],\n",
       "       [0.13949386, 0.29214465, 0.36636184],\n",
       "       [0.45606998, 0.78517596, 0.19967378],\n",
       "       [0.51423444, 0.59241457, 0.04645041]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_T = np.random.rand(10,3)\n",
    "X_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias feature\n",
    "dummy = np.ones(len(X_T))\n",
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.37454012, 0.95071431, 0.73199394],\n",
       "       [1.        , 0.59865848, 0.15601864, 0.15599452],\n",
       "       [1.        , 0.05808361, 0.86617615, 0.60111501],\n",
       "       [1.        , 0.70807258, 0.02058449, 0.96990985],\n",
       "       [1.        , 0.83244264, 0.21233911, 0.18182497],\n",
       "       [1.        , 0.18340451, 0.30424224, 0.52475643],\n",
       "       [1.        , 0.43194502, 0.29122914, 0.61185289],\n",
       "       [1.        , 0.13949386, 0.29214465, 0.36636184],\n",
       "       [1.        , 0.45606998, 0.78517596, 0.19967378],\n",
       "       [1.        , 0.51423444, 0.59241457, 0.04645041]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_T = np.column_stack((dummy, X_T))\n",
    "X_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate labels for the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.random.choice([0,1],(len(X_T)))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate weights for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80839735, 0.30461377, 0.09767211, 0.68423303])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(len(X_T[0]))\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W <sup>T</sup> X (or equivalently X<sup>T</sup>W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51620013, 1.11273224, 1.32199442, 1.68974089, 1.2071211 ,\n",
       "       1.25303655, 1.38706867, 1.13010036, 1.16063574, 1.05468553])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_T_X = np.dot(w, X_T.T)\n",
    "w_T_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.28130311, 1.48955766, 1.36351809, 1.22634284, 1.42664952,\n",
       "       1.39984683, 1.33298942, 1.47710671, 1.45621231, 1.53445294])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(w_T_X):\n",
    "    return 1/ ( 1 - np.exp(-1. *w_T_X) )\n",
    "\n",
    "h_theta = sigmoid(w_T_X)\n",
    "h_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient dW\n",
    "Turns out that the gradients wrt the weights in logistic regression are identical to the gradients of linear regression except for the $h_{\\theta}$.\n",
    "\n",
    "### $h_{\\theta}$ in linear regression was w<sup>T</sup>.X\n",
    "<br>\n",
    "\n",
    "### $h_{\\theta}$ in logistic regression is ${\\sigma}$ (w <sup> T</sup>.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.98797941, 4.41289043, 4.28475592, 4.5093801 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = (h_theta - labels).dot(X_T)\n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights before gradient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80839735, 0.30461377, 0.09767211, 0.68423303])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights after gradient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80739855, 0.30417248, 0.09724364, 0.68378209])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "w = w - lr * dw\n",
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
